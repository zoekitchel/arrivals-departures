---
title: "Temp_Data_All_Years"
output: html_notebook
---

```{r setup}
library(dplyr)
library(data.table)
library(here)
library(raster)
library(ncdf4)
library(lubridate)
library(maptools)
library(rgdal)
library(tidyr)
library(ggplot2)

load("/Users/zoekitchel/Documents/grad school/Rutgers/Coursework/Fall 2018/SDM/hauls_catch_Dec2017 1.RData")
```

First, I will make list of GPS points and days/year ranges for which I need temperatures

```{r}
latlongyear <- hauls %>%
  dplyr::select(lat, lon, year, month)

```

Okay, now getting SODA data
Where I got the data to match soda in trawlData (following Ryan's readme)
Soda Data Files

 1. Go Here: http://iridl.ldeo.columbia.edu/SOURCES/.CARTON-GIESE/.SODA/.v2p1p6/.temp/time/%28Jan%201958%29%28Dec%202008%29RANGEEDGES/lat/%280N%29%2889.5N%29RANGEEDGES/lon/%28-200E%29%2820E%29RANGEEDGES/time/%28Jan%201958%29%28Dec%202008%29RANGEEDGES/depth/%285.01%29%285.01%29RANGEEDGES/lat/%280N%29%2889.5N%29RANGEEDGES/lon/%28-200E%29%2820E%29RANGEEDGES/datafiles.html

 2. Restrict ranges as follows for SST to match SODA in trawlData

 	-200E to 20E --> 440 pts

 	0N to 89.5N --> 179 pts

 	5.01 to 5.01

 	Jan 1958 to Dec 2008 --> 612 pts


 3. Stop Selecting

 4. Click Data Files

 5. Download NetCDF Format
 6. File is too big for GitHub, so compress, mine compressed to 95mb using Keka to 7zip

```{r}
cp <- nc_open("/Users/zoekitchel/Documents/grad school/Rutgers/LabWork/Colonization_Extinction/CARTON-GIESE_SODA_v2p1p6_sstemp.nc")
print(cp)

# =========================================
# = Function to Read in SODA, Grab Surface =
# =========================================
get.soda.sst <- function(file){

	soda.info <- nc_open(file)
	name.soda.sizes <- sapply(soda.info$var$temp$dim, function(x)x$name)
	soda.sizes <- soda.info$var$temp$size
	dim.units <- sapply(soda.info$var$temp$dim, function(x)x$units)
	print(dim.units)
	stopifnot(grepl("months since ", dim.units[4])) # make sure time is in correct units and in right place
	names(soda.sizes) <- name.soda.sizes
	ntime <- soda.sizes["time"]
	ndepth <- soda.sizes["depth"]

	soda.time0 <- soda.info$var$temp$dim[[4]]$vals
	ref.date <- as.Date(gsub("months since ", "", dim.units[4]))
	start.before.ref <- grepl("-", soda.time0[1]) # is the first date before ref.date?
	n.month.before <- ceiling(abs(soda.time0[1])) + as.integer(start.before.ref)
	start.increment <- ifelse(start.before.ref, "-1 month", "1 month")
	time.start <- rev(seq.Date(ref.date, by=start.increment, length.out=n.month.before))[1]
	soda.time <- seq.Date(time.start, by="1 month", length.out=ntime)
	
	soda.sst <- brick(file)
	names(soda.sst) <- soda.time

		
	return(soda.sst)
	
}


## Using Ryan's code
soda_sst <- get.soda.sst("/Users/zoekitchel/Documents/grad school/Rutgers/LabWork/Colonization_Extinction/CARTON-GIESE_SODA_v2p1p6_sstemp.nc")
soda_sst

plot(soda_sst[[9]])

saveRDS(soda_sst, "SODA2.1.6_SST.rds")
```
New column in latlongyear with "X1958.01.01" format, looks like I may not need to do this after all...

```{r}
latlongyear$month_digits <- NA
for (i in 1:nrow(latlongyear)) {
  if (latlongyear$month[[i]] < 10) {
    latlongyear$month_digits[[i]] <- paste0("0", month(latlongyear$month[[i]]))
  } else {
    latlongyear$month_digits[[i]] <- paste0(month(latlongyear$month[[i]]))
  }
}
latlongyear$day <- paste0("0", c(1))
latlongyear$year_digits <- paste0("X", latlongyear$year)
latlongyear$ID <- paste(latlongyear$year_digits, latlongyear$month_digits, latlongyear$day, sep=".")
latlongyearIDonly <- latlongyear %>%
  dplyr::select(ID, lat, lon)
justlatlong <- latlongyear %>%
  dplyr::select(lat, lon)
```
SET COORDINATE SYSTEM
```{r}
locations <- SpatialPoints(cbind(latlongyear$lon, latlongyear$lat), 
                            proj4string=CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"))
cord.UTM <- spTransform(locations, crs(soda_sst))
plot(cord.UTM)
```

Now, let's extract values

```{r}
temp_values <- raster::extract(soda_sst, cord.UTM)
df <- cbind(justlatlong, temp_values)
subset(df, is.na(df[,4])) #looking for NA's
```
Let's look at the problem rows, GOOD NEWS, the problem is that we have a few GPS locations of trawls lacking datapoints, luckily, we can just get rid of these points when averaging!!
```{r}
#now, to make my life easier, I'm just gonna remove NA's here
df_na_rm <- df[complete.cases(df[,4]), ]
head(df_na_rm, rows=10, cols=10)
df_long <- gather(df_na_rm, key = "ID", value = "temp", 3:614 )
head(df_long)
```

We now have to split the date back up unfortunately, and get rid of X

```{r}
rm(df, soda_sst, cord.UTM, df_na_rm, locations, temp_values)#open memory
df_long2 <- df_long %>%
  separate(col = ID, into = c("year", "month", "day"))

df_long2$year <- substr(as.character(df_long2$year), 2, 5)
head(df_long2, 30)
save(df_long2, file = "SODA_monthly_data.R")
```

I currently have mean monthly values for a ton of GPS points within a region.
For temperature values, I will first take a mean for each month over the whole region, so then I have a mean month/year value. region_*month*_mean_sst
Then, I will take mean of mean month/year of year of trawl to get a YEARLY mean to represent the whole region. region_year_mean_sst
For maximum temperature experienced in a year, I will take maximum of mean month/year and for minimum I will take minimum of mean month/year. region_year_max_sst, region_year_min_sst
I intend to look at 10 years of possible lags. 

Therefore, the next step is to merge these data with which regions link to which GPS points using hauls$region

First, I will make a lat lon region key

```{r}
latlongreg <- hauls %>%
  dplyr::select(lat, lon, region)
head(latlongreg)
```

Then I will link data table with GPS and date and temp with regions using above key, and then take appropriate averages

```{r}
date_temp_reg <- left_join(df_long2, latlongreg, by = c("lat", "lon"))
head(df_long2)
date_temp_reg_month_avg <- date_temp_reg %>%
  group_by(region, year, month) %>%
  summarize(region_month_mean_sst = mean(temp))
head(date_temp_reg_month_avg)

date_temp_reg_avg <- date_temp_reg_month_avg %>%
  group_by(region, year) %>%
  summarize(region_year_mean_sst = mean(region_month_mean_sst), region_year_max_sst = max(region_month_mean_sst), region_year_min_sst = min(region_month_mean_sst)) %>%
  mutate(region_year_seas_sst = region_year_max_sst - region_year_min_sst)
head(date_temp_reg_avg)

save(date_temp_reg_avg, date_temp_reg_month_avg, file = "date_ssttemp_avg_SODA.Rdata") #save this so i can start here!

ggplot(data=date_temp_reg_avg, aes(x=year, y=region_year_mean_sst)) +
  geom_point() +
  facet_wrap(~region)
base::nrow(date_temp_reg_avg)
```

I now have the predictor values I need, yearly min and max for entire region per year

Next, I need to link to regional yearly species data

```{r}
load("spp_master.RData")
#compare temps I calculated to temperatures already there
#match region names
date_temp_reg_avg$region <- as.factor(date_temp_reg_avg$region)
levels(date_temp_reg_avg$region)[levels(date_temp_reg_avg$region)=="AFSC_Aleutians"] <- "ai"
levels(date_temp_reg_avg$region)[levels(date_temp_reg_avg$region)=="AFSC_EBS"] <- "ebs"
levels(date_temp_reg_avg$region)[levels(date_temp_reg_avg$region)=="AFSC_GOA"] <- "goa"
levels(date_temp_reg_avg$region)[levels(date_temp_reg_avg$region)=="AFSC_WCTri"] <- "wctri"
levels(date_temp_reg_avg$region)[levels(date_temp_reg_avg$region)=="DFO_Newfoundland"] <- "newf"
levels(date_temp_reg_avg$region)[levels(date_temp_reg_avg$region)=="DFO_ScotianShelf"] <- "shelf"
levels(date_temp_reg_avg$region)[levels(date_temp_reg_avg$region)=="NEFSC_NEUS"] <- "neus"
levels(date_temp_reg_avg$region)[levels(date_temp_reg_avg$region)=="SCDNR_SEUS"] <- "se"
levels(date_temp_reg_avg$region)[levels(date_temp_reg_avg$region)=="SEFSC_GOMex"] <- "gmex"

spp_master_ztemp <- left_join(spp_master, date_temp_reg_avg, by = c("region", "year"))
spp_master$temp_dif <- spp_master_ztemp$temp-spp_master_ztemp$ztemps
#hopefully they match up!
head(date_temp_reg_avg)
```
Next step is to calculate change in temp from previous year to now, luckily I can farm this code from 27_acf_timeseries_analysis_covariates.R

I think I should calculate both change in temperature and lags from date_temp_reg_avg in order to keep years not actually surveyed (i.e. triennial west coast survey)

Here I am first adding lags
```{r}
#adding lag values (1, 2, 3, 4, 5) for seasonality and change in temperature
nm1 <- grep("*sst*", colnames(date_temp_reg_avg), value=TRUE)
nm2 <- paste("lag1", nm1, sep=".")
nm3 <- paste("lag2", nm1, sep=".")
nm4 <- paste("lag3", nm1, sep=".")
nm5 <- paste("lag4", nm1, sep=".")
nm6 <- paste("lag5", nm1, sep=".")
nm7 <- paste("lag6", nm1, sep=".")
nm8 <- paste("lag7", nm1, sep=".")
nm9 <- paste("lag8", nm1, sep=".")
nm10 <- paste("lag9", nm1, sep=".")
nm11 <- paste("lag10", nm1, sep=".")
date_temp_reg_avg_withlags <- data.table(date_temp_reg_avg)
date_temp_reg_avg_withlags[, (nm2) :=  shift(.SD, n=1, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm3) :=  shift(.SD, n=2, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm4) :=  shift(.SD, n=3, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm5) :=  shift(.SD, n=4, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm6) :=  shift(.SD, n=5, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm7) :=  shift(.SD, n=6, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm8) :=  shift(.SD, n=7, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm9) :=  shift(.SD, n=8, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm10) :=  shift(.SD, n=9, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm11) :=  shift(.SD, n=10, type = "lag"), by=region, .SDcols=nm1]
```

Now, I'm adding change since last year
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change") := (region_year_mean_sst - lag1.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change") := (region_year_max_sst - lag1.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change") := (region_year_min_sst - lag1.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change") := (region_year_seas_sst - lag1.region_year_seas_sst)]

```

Another way to look at lags

```{r}
ccf()
```
