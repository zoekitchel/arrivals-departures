---
title: "Temp_Data_All_Years"
output: html_notebook
---

```{r setup}
library(dplyr)
library(data.table)
library(raster)
library(ncdf4)
library(lubridate)
library(maptools)
library(rgdal) #can't install into terminal R 3.6
library(tidyr)
library(ggplot2)

load("/Users/zoekitchel/Documents/grad school/Rutgers/Coursework/Fall 2018/SDM/hauls_catch_Dec2017 1.RData")
#load("col_ext/hauls_catch_Dec2017 1.RData")
```

First, I will make list of GPS points and days/year ranges for which I need temperatures, keeping in mind I want to be able to look at lags of 10, and therefore I need to extend time back by 10.

```{r compiling lat, long, year}
latlongyear <- hauls %>%
  dplyr::select(lat, lon, year, region)

#add years back to 1953
#for each group, find first year, then generate new observations with repeated GPS sequence going back 10 years

#regions
regions <- levels(as.factor(hauls$region))

newlatlongyear <- data.table(matrix(ncol = 5, nrow = 0))

colnames(newlatlongyear) <- c("lat", "lon", "year", "month", "region")

for (i in 1:length(regions)) {
  subset <- latlongyear[latlongyear$region == regions[i],] #subset to a region, ct 4588
  minyear <- min(subset$year) #minimum year for this region
  newminyear <- minyear - 10 #minimum year minus 10 (to allow for lags)
  secondtomin <- min(subset$year[subset$year!=min(subset$year)]) #year of second earliest tow
  maxyear <- max(subset$year) #year of latest tow
  #secondtomax <- max(subset$year[subset$year!=max(subset$year)]) #year of second latest tow
  
  allyears <- seq(newminyear, maxyear) #years we need to simulate GPS points for, 19
  
  #subsetlatlong <- subset[subset$year <= secondtomin | subset$year >= secondtomax,] #subset lat lon values used in first 2 years and last two years of survey
  subsetlatlong.noreg <- subset[,c(1:2)] #don't need column with location or year anymore, 1488
  #randomly choose 1/5 of these
  twentieth <- 0.05*nrow(subsetlatlong.noreg)
  subsetlatlong.reduced <- data.table(subsetlatlong.noreg[sample(nrow(subsetlatlong.noreg), twentieth), ]) #229 GPS points
  
  allmonths <- c(1:12) #we want temp values for every month
  
  subsetlatlong.m <- subsetlatlong.reduced[ , latlon := do.call(paste, c(.SD, sep = "_"))] #merge lat lon in order to expand grid (essentially, lat_lon becomes a key)
  
  subsetlatlong.m.vector <- subsetlatlong.m$latlon #extract vector
  
  #we now have three vectors we need to expand grid for, subsetlatlong.m.vector, allyears, and allmonths, length of expanded grid should equal
  invisible(length(subsetlatlong.m.vector)*length(allyears)*length(allmonths)) #this checks out
  
  tempdf <- data.table(expand.grid(subsetlatlong.m.vector, allyears, allmonths)) #matches every year value with a lat_lon_month
  
  tempdf.sep <- tempdf[, c("lat", "lon") := tstrsplit(Var1, "_", fixed=TRUE)]
  colnames(tempdf.sep)[colnames(tempdf.sep)=="Var2"] <- "year"
  colnames(tempdf.sep)[colnames(tempdf.sep)=="Var3"] <- "month"
  tempdf.sep$region <- regions[i]
  tempdf.sep <- tempdf.sep[,Var1:=NULL] #delete column we split from
  
  newlatlongyear <- rbind(newlatlongyear, tempdf.sep) #this file now has ALL possible year, lat, lon, month points we need to extract
  
} # this for loop generates latitude year combinations for 10 years before sampling began to allow us to look at lags up to 10 years with no NA's

#it's possible at some point I may need to randomly sample these, because 8736684 extractions may be too many... and I was correct
#we know it works okay with 136044 hauls, let's take a sub-sample of this, and then multiply by years (~70) and months (~12) (840)


```

Okay, now getting SODA data
Where I got the data to match soda in trawlData (following Ryan's readme)
Soda Data Files

 1. [Go Here](https://iridl.ldeo.columbia.edu/SOURCES/.CARTON-GIESE/.SODA/.v2p2p4/.temp/time/%28Jan%201953%29%28Dec%202008%29RANGEEDGES/depth/%285.01%29%285.01%29RANGEEDGES/lat/%280N%29%2889.5N%29RANGEEDGES/lon/%28-200E%29%2820E%29RANGEEDGES/datafiles.html)

 2. Restrict ranges as follows for SST to match SODA in trawlData

 	-200E to 20E --> 440 pts

 	0N to 89.5N --> 179 pts

 	5.01 to 5.01

 	Jan 1958 to Dec 2008 --> 672 pts


 3. Stop Selecting

 4. Click Data Files

 5. Download NetCDF Format
 6. File is too big for GitHub, so compress, mine compressed to 95mb using Keka to 7zip

```{r import SODA temperature data}
cp <- nc_open("/Users/zoekitchel/Documents/grad school/Rutgers/LabWork/Colonization_Extinction/CARTON-GIESE_SODA_v2p1p6_sstemp.nc")
print(cp)

# =========================================
# = Function to Read in SODA, Grab Surface =
# =========================================
get.soda.sst <- function(file){

	soda.info <- nc_open(file)
	name.soda.sizes <- sapply(soda.info$var$temp$dim, function(x)x$name)
	soda.sizes <- soda.info$var$temp$size
	dim.units <- sapply(soda.info$var$temp$dim, function(x)x$units)
	print(dim.units)
	stopifnot(grepl("months since ", dim.units[4])) # make sure time is in correct units and in right place
	names(soda.sizes) <- name.soda.sizes
	ntime <- soda.sizes["time"]
	ndepth <- soda.sizes["depth"]

	soda.time0 <- soda.info$var$temp$dim[[4]]$vals
	ref.date <- as.Date(gsub("months since ", "", dim.units[4]))
	start.before.ref <- grepl("-", soda.time0[1]) # is the first date before ref.date?
	n.month.before <- ceiling(abs(soda.time0[1])) + as.integer(start.before.ref)
	start.increment <- ifelse(start.before.ref, "-1 month", "1 month")
	time.start <- rev(seq.Date(ref.date, by=start.increment, length.out=n.month.before))[1]
	soda.time <- seq.Date(time.start, by="1 month", length.out=ntime)
	
	soda.sst <- brick(file)
	names(soda.sst) <- soda.time

		
	return(soda.sst)
	
}

## Using Ryan's code
soda_sst <- get.soda.sst("/Users/zoekitchel/Documents/grad school/Rutgers/LabWork/Colonization_Extinction/CARTON-GIESE_SODA_v2p1p6_sstemp.nc")
soda_sst

plot(soda_sst[[9]])

saveRDS(soda_sst, "SODA2.1.6_SST.rds")
#load("col_ext/SODA2.1.6_SST.rds")
```

New column in newlatlongyear with "X1958.01.01" format, looks like I may not need to do this after all... SKIPPING FOR NOW
*this takes a super long time to run, so we should save this result
```{r}
newlatlongyear$month_digits <- NA
for (i in 1:nrow(newlatlongyear)) {
  if (newlatlongyear$month[[i]] < 10) {
    newlatlongyear$month_digits[[i]] <- paste0("0", newlatlongyear$month[[i]])
  } else {
    newlatlongyear$month_digits[[i]] <- paste0(newlatlongyear$month[[i]])
  }
}
newlatlongyear$day <- paste0("0", c(1))
newlatlongyear$year_digits <- paste0("X", newlatlongyear$year)
newlatlongyear$ID <- paste(newlatlongyear$year_digits, newlatlongyear$month_digits, newlatlongyear$day, sep=".")

newlatlongyear <- transform(newlatlongyear, lat = as.numeric(lat), lon = as.numeric(lon))

newlatlongyearIDonly <- newlatlongyear %>%
  dplyr::select(ID, lat, lon)
latlongonly <- newlatlongyear %>%
  dplyr::select(lat, lon)

save(newlatlongyear, newlatlongyearIDonly, file = "newlatlongyear.Rdata")

```

SET COORDINATE SYSTEM
```{r set coordinate system}
locations <- SpatialPoints(cbind(newlatlongyear$lon, newlatlongyear$lat), 
                            proj4string=CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"))
plot(locations)
```

Now, let's extract values 
*NB: if they do a correct sampling method, this shouldn't be a problem, but is there any world in which we should be averaging temperature values in a different way? here, we're just using the trawl data points, should it be rather the whole region? They seem to be rather arbitrary break points?*

#this also takes a long time, so be sure to save output
```{r extracting temperature values at correct GPS points}
temp_values <- raster::extract(soda_sst, locations)
save(temp_values, file = "extracted_temp_values_SODA.Rdata")

justlatlong <- newlatlongyear %>%
  dplyr::select(lat, lon)

df <- cbind(justlatlong, temp_values)
subset(df, is.na(df[,4])) #looking for NA's
```
Let's look at the problem rows, GOOD NEWS, the problem is that we have a few GPS locations of trawls lacking datapoints, luckily, we can just get rid of these points when averaging!!
```{r}
#now, to make my life easier, I'm just gonna remove NA's here
df_na_rm <- df[complete.cases(df[,4]), ]
head(df_na_rm, rows=10, cols=10)
df_long <- gather(df, key = "ID", value = "temp", 3:674 )
head()
```

We now have to split the date back up unfortunately, and get rid of X

```{r}
rm(df, soda_sst, cord.UTM, df_na_rm, locations, temp_values)#open memory
df_long2 <- df_long %>%
  separate(col = ID, into = c("year", "month", "day"))

df_long2$year <- substr(as.character(df_long2$year), 2, 5)
head(df_long2, 30)
save(df_long2, file = "SODA_monthly_data.R")
```

I currently have mean monthly values for a ton of GPS points within a region.
For temperature values, I will first take a mean for each month over the whole region, so then I have a mean month/year value. region_*month*_mean_sst
Then, I will take mean of mean month/year of year of trawl to get a YEARLY mean to represent the whole region. region_year_mean_sst
For maximum temperature experienced in a year, I will take maximum of mean month/year and for minimum I will take minimum of mean month/year. region_year_max_sst, region_year_min_sst
I intend to look at 10 years of possible lags. 

Therefore, the next step is to merge these data with which regions link to which GPS points using hauls$region

First, I will make a lat lon region key

```{r}
latlongreg.key <- latlongyear[,c(1,2,5)]
```

Then I will link data table with GPS and date and temp with regions using above key, and then take appropriate averages

```{r}
date_temp_reg <- left_join(df_long2, latlongreg.key, by = c("lat", "lon"))
date_temp_reg_month_avg <- date_temp_reg %>%
  group_by(region, year, month) %>%
  summarize(region_month_mean_sst = mean(temp))
head(date_temp_reg_month_avg)

date_temp_reg_avg <- date_temp_reg_month_avg %>%
  group_by(region, year) %>%
  summarize(region_year_mean_sst = mean(region_month_mean_sst), region_year_max_sst = max(region_month_mean_sst), region_year_min_sst = min(region_month_mean_sst)) %>%
  mutate(region_year_seas_sst = region_year_max_sst - region_year_min_sst)
head(date_temp_reg_avg)

save(date_temp_reg_avg, date_temp_reg_month_avg, file = "date_ssttemp_avg_SODA.Rdata") #save this so i can start here!

ggplot(data=date_temp_reg_avg, aes(x=year, y=region_year_mean_sst)) +
  geom_point() +
  facet_wrap(~region)
base::nrow(date_temp_reg_avg)
```

I now have the predictor values I need, yearly min and max for entire region per year

Next step is to calculate change in temp from previous year to now, luckily I can farm this code from 27_acf_timeseries_analysis_covariates.R

I think I should calculate both change in temperature and lags from date_temp_reg_avg in order to keep years not actually surveyed (i.e. triennial west coast survey)

Here I am first adding lags
```{r}
#adding lag values (1, 2, 3, 4, 5) for seasonality and change in temperature
nm1 <- grep("*sst*", colnames(date_temp_reg_avg), value=TRUE)
nm2 <- paste("lag1", nm1, sep=".")
nm3 <- paste("lag2", nm1, sep=".")
nm4 <- paste("lag3", nm1, sep=".")
nm5 <- paste("lag4", nm1, sep=".")
nm6 <- paste("lag5", nm1, sep=".")
nm7 <- paste("lag6", nm1, sep=".")
nm8 <- paste("lag7", nm1, sep=".")
nm9 <- paste("lag8", nm1, sep=".")
nm10 <- paste("lag9", nm1, sep=".")
nm11 <- paste("lag10", nm1, sep=".")
date_temp_reg_avg_withlags <- data.table(date_temp_reg_avg)
date_temp_reg_avg_withlags[, (nm2) :=  data.table::shift(.SD, n=1, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm3) :=  data.table::shift(.SD, n=2, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm4) :=  data.table::shift(.SD, n=3, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm5) :=  data.table::shift(.SD, n=4, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm6) :=  data.table::shift(.SD, n=5, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm7) :=  data.table::shift(.SD, n=6, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm8) :=  data.table::shift(.SD, n=7, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm9) :=  data.table::shift(.SD, n=8, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm10) :=  data.table::shift(.SD, n=9, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm11) :=  data.table::shift(.SD, n=10, type = "lag"), by=region, .SDcols=nm1]
```

Now, I'm adding change since last year
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change") := (region_year_mean_sst - lag1.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change") := (region_year_max_sst - lag1.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change") := (region_year_min_sst - lag1.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change") := (region_year_seas_sst - lag1.region_year_seas_sst)]
```
Change experienced last year
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag1") := (lag1.region_year_mean_sst - lag2.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag1") := (lag1.region_year_mean_sst - lag2.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag1") := (lag1.region_year_mean_sst - lag2.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag1") := (lag1.region_year_mean_sst - lag2.region_year_seas_sst)]
```
Change experienced 2 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag2") := (lag2.region_year_mean_sst - lag3.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag2") := (lag2.region_year_mean_sst - lag3.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag2") := (lag2.region_year_mean_sst - lag3.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag2") := (lag2.region_year_mean_sst - lag3.region_year_seas_sst)]
```
Change experienced 3 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag3") := (lag3.region_year_mean_sst - lag4.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag3") := (lag3.region_year_mean_sst - lag4.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag3") := (lag3.region_year_mean_sst - lag4.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag3") := (lag3.region_year_mean_sst - lag4.region_year_seas_sst)]
```
Change experienced 4 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag4") := (lag4.region_year_mean_sst - lag5.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag4") := (lag4.region_year_mean_sst - lag5.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag4") := (lag4.region_year_mean_sst - lag5.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag4") := (lag4.region_year_mean_sst - lag5.region_year_seas_sst)]
```
Change experienced 5 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag5") := (lag5.region_year_mean_sst - lag6.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag5") := (lag5.region_year_mean_sst - lag6.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag5") := (lag5.region_year_mean_sst - lag6.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag5") := (lag5.region_year_mean_sst - lag6.region_year_seas_sst)]
```
Change experienced 6 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag6") := (lag6.region_year_mean_sst - lag7.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag6") := (lag6.region_year_mean_sst - lag7.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag6") := (lag6.region_year_mean_sst - lag7.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag6") := (lag6.region_year_mean_sst - lag7.region_year_seas_sst)]
```
Change experienced 7 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag7") := (lag7.region_year_mean_sst - lag8.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag7") := (lag7.region_year_mean_sst - lag8.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag7") := (lag7.region_year_mean_sst - lag8.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag7") := (lag7.region_year_mean_sst - lag8.region_year_seas_sst)]
```
Change experienced 8 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag8") := (lag8.region_year_mean_sst - lag9.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag8") := (lag8.region_year_mean_sst - lag9.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag8") := (lag8.region_year_mean_sst - lag9.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag8") := (lag8.region_year_mean_sst - lag9.region_year_seas_sst)]
```
Change experienced 9 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag9") := (lag9.region_year_mean_sst - lag10.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag9") := (lag9.region_year_mean_sst - lag10.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag9") := (lag9.region_year_mean_sst - lag10.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag9") := (lag9.region_year_mean_sst - lag10.region_year_seas_sst)]
```
From Ryan's famous plot, we are more concerned with absolute value of change!
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_abs") := abs(sst_mean_change)]
date_temp_reg_avg_withlags[,c("sst_max_change_abs") := abs(sst_max_change)]
date_temp_reg_avg_withlags[,c("sst_min_change_abs") := abs(sst_min_change)]
date_temp_reg_avg_withlags[,c("sst_seas_change_abs") := abs(sst_seas_change)]
```
Abs Change experienced last year
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag1_abs") := abs(lag1.region_year_mean_sst - lag2.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag1_abs") := abs(lag1.region_year_max_sst - lag2.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag1_abs") := abs(lag1.region_year_min_sst - lag2.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag1_abs") := abs(lag1.region_year_seas_sst - lag2.region_year_seas_sst)]
```
Abs Change experienced 2 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag2_abs") := abs(lag2.region_year_mean_sst - lag3.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag2_abs") := abs(lag2.region_year_max_sst - lag3.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag2_abs") := abs(lag2.region_year_min_sst - lag3.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag2_abs") := abs(lag2.region_year_seas_sst - lag3.region_year_seas_sst)]
```
Abs Change experienced 3 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag3_abs") := abs(lag3.region_year_mean_sst - lag4.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag3_abs") := abs(lag3.region_year_max_sst - lag4.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag3_abs") := abs(lag3.region_year_min_sst - lag4.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag3_abs") := abs(lag3.region_year_seas_sst - lag4.region_year_seas_sst)]
```
Abs Change experienced 4 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag4_abs") := abs(lag4.region_year_mean_sst - lag5.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag4_abs") := abs(lag4.region_year_max_sst - lag5.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag4_abs") := abs(lag4.region_year_min_sst - lag5.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag4_abs") := abs(lag4.region_year_seas_sst - lag5.region_year_seas_sst)]
```
Abs Change experienced 5 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag5_abs") := abs(lag5.region_year_mean_sst - lag6.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag5_abs") := abs(lag5.region_year_max_sst - lag6.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag5_abs") := abs(lag5.region_year_min_sst - lag6.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag5_abs") := abs(lag5.region_year_seas_sst - lag6.region_year_seas_sst)]
```
Abs Change experienced 6 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag6_abs") := abs(lag6.region_year_mean_sst - lag7.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag6_abs") := abs(lag6.region_year_max_sst - lag7.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag6_abs") := abs(lag6.region_year_min_sst - lag7.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag6_abs") := abs(lag6.region_year_seas_sst - lag7.region_year_seas_sst)]
```
Abs Change experienced 7 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag7_abs") := abs(lag7.region_year_mean_sst - lag8.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag7_abs") := abs(lag7.region_year_max_sst - lag8.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag7_abs") := abs(lag7.region_year_min_sst - lag8.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag7_abs") := abs(lag7.region_year_seas_sst - lag8.region_year_seas_sst)]
```
Abs Change experienced 8 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag8_abs") := abs(lag8.region_year_mean_sst - lag9.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag8_abs") := abs(lag8.region_year_max_sst - lag9.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag8_abs") := abs(lag8.region_year_min_sst - lag9.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag8_abs") := abs(lag8.region_year_seas_sst - lag9.region_year_seas_sst)]
```
Let's git rid of any rows with any NA's (any important ones shouldn't have any) and then save all these goodies!!
```{r}
date_temp_reg_avg_withlags.naomit <- na.omit(date_temp_reg_avg_withlags)
save(date_temp_reg_avg_withlags.naomit, file = "date_ssttemp_avg_SODA_withlags.Rdata")
```

Another way to look at lags, but this isn't appropriate for the binomials I'm currently looking at

```{r}
ccf()
```
