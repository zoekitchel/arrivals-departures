---
title: "Temp_Data_All_Years"
output: html_notebook
---

```{r setup}
library(dplyr)
library(data.table)
library(raster)
library(ncdf4)
library(lubridate)
library(maptools)
library(rgdal)
library(tidyr)
library(ggplot2)

load("/Users/zoekitchel/Documents/grad school/Rutgers/Coursework/Fall 2018/SDM/hauls_catch_Dec2017 1.RData")
#load("col_ext/hauls_catch_Dec2017 1.RData")
```

First, I will make list of GPS points and days/year ranges for which I need temperatures, keeping in mind I want to be able to look at lags of 10, and therefore I need to extend time back by 10.

```{r compiling lat, long, year}
latlongyear <- hauls %>%
  dplyr::select(lat, lon, year, region)

latlongyear <- as.data.table(latlongyear)


```
I am having way too many issues with such a giant data set. Let's try another route
```{r alternative route}
#How many lat/lon combos for each year/region?
regions <- levels(as.factor(hauls$region))

unique_combos <- vector()
for (i in 1:length(regions)) {
  subset <- latlongyear %>% filter(region == regions[i])
  
  years <- levels(as.factor(subset$year))
  
  for (j in 1:length(years)) {
    subset2 <- subset %>% filter(year == years[j])
    uniquelatlong <- nrow(unique(subset2[,c('lat','lon')]))
    unique_combos[length(unique_combos)+1] <- uniquelatlong
  }
}

mean(unique_combos) #avg number lat/long per year/reg combo is ~450

#does each year look similar? yes, checked for a few regions
levels(as.factor(latlongyear[region == regions[8]]$year))
plot(x=latlongyear[region == regions[8] & year == 2012]$lon, y=latlongyear[region == regions[8] & year == 2012]$lat)
points(x=latlongyear[region == regions[8] & year == 1986]$lon, y=latlongyear[region == regions[8] & year == 1986]$lat, col = "blue")
points(x=latlongyear[region == regions[8] & year == 1991]$lon, y=latlongyear[region == regions[8] & year == 1991]$lat, col = "red")
points(x=latlongyear[region == regions[8] & year == 1994]$lon, y=latlongyear[region == regions[8] & year == 1994]$lat, col = "green")

```
So, I need roughly ~600 points per year/month combo, so, 4 months x years x 600 points = 24000-127200

Below, I am reducing to a mere 0.01 of all points, or 45 points per month/year/reg combo, because I think that's all I can handle, but it still seems representative
```{r compiling lat, long, year, month to extract }

#add years back to 1953
#for each group, find first year, then generate new observations with repeated GPS sequence going back 10 years

#regions
regions <- levels(as.factor(hauls$region))

newlatlongyear <- data.table(matrix(ncol = 5, nrow = 0))

colnames(newlatlongyear) <- c("lat", "lon", "year", "month", "region")

for (i in 1:length(regions)) {
  subset <- latlongyear[latlongyear$region == regions[i],] #subset to a region, ct 4588
  minyear <- min(subset$year) #minimum year for this region
  newminyear <- minyear - 10 #minimum year minus 10 (to allow for lags)
  secondtomin <- min(subset$year[subset$year!=min(subset$year)]) #year of second earliest tow
  maxyear <- max(subset$year) #year of latest tow
  #secondtomax <- max(subset$year[subset$year!=max(subset$year)]) #year of second latest tow
  
  allyears <- seq(newminyear, maxyear) #years we need to simulate GPS points for, 42
  
  #subsetlatlong <- subset[subset$year <= secondtomin | subset$year >= secondtomax,] #subset lat lon values used in first 2 years and last two years of survey
  subsetlatlong.noreg <- subset[,c(1:2)] #don't need column with location or year anymore, 4588
  #above gives me all GPS points that were ever sampled in this region over entire time period
  #randomly choose 1/5 of these
  hundredth <- 0.01*nrow(subsetlatlong.noreg)
  subsetlatlong.reduced <- data.table(subsetlatlong.noreg[sample(nrow(subsetlatlong.noreg), hundredth), ]) #only 45 points
  
  allmonths <- c(1,2,7,8) #we want temp values for 2 coldest and 2 warmest months
  
  subsetlatlong.m <- subsetlatlong.reduced[ , latlon := do.call(paste, c(.SD, sep = "_"))] #merge lat lon in order to expand grid (essentially, lat_lon becomes a key)
  
  subsetlatlong.m.vector <- subsetlatlong.m$latlon #extract vector
  
  #we now have three vectors we need to expand grid for, subsetlatlong.m.vector, allyears, and allmonths, length of expanded grid should equal
  invisible(length(subsetlatlong.m.vector)*length(allyears)*length(allmonths)) #this checks out
  
  tempdf <- data.table(expand.grid(subsetlatlong.m.vector, allyears, allmonths)) #matches every year value with a lat_lon_month, 7560 in total
  
  tempdf.sep <- tempdf[, c("lat", "lon") := tstrsplit(Var1, "_", fixed=TRUE)]
  colnames(tempdf.sep)[colnames(tempdf.sep)=="Var2"] <- "year"
  colnames(tempdf.sep)[colnames(tempdf.sep)=="Var3"] <- "month"
  tempdf.sep$region <- regions[i]
  tempdf.sep <- tempdf.sep[,Var1:=NULL] #delete column we split from
  
  newlatlongyear <- rbind(newlatlongyear, tempdf.sep) #this file now has ALL possible year, lat, lon, month points we need to extract
  
} # this for loop generates latitude year combinations for 10 years before sampling began to allow us to look at lags up to 10 years with no NA's

#it's possible at some point I may need to randomly sample these, because 8736684 extractions may be too many... and I was correct
#we know it works okay with 136044 hauls, let's take a sub-sample of this, and then multiply by years (~70) and months (~12) (840)


```

Okay, now getting SODA data
Where I got the data to match soda in trawlData (following Ryan's readme)
Soda Data Files

 1. [Go Here](https://iridl.ldeo.columbia.edu/SOURCES/.CARTON-GIESE/.SODA/.v2p2p4/.temp/time/%28Jan%201953%29%28Dec%202008%29RANGEEDGES/depth/%285.01%29%285.01%29RANGEEDGES/lat/%280N%29%2889.5N%29RANGEEDGES/lon/%28-200E%29%2820E%29RANGEEDGES/datafiles.html)

 2. Restrict ranges as follows for SST to match SODA in trawlData

 	-200E to 20E --> 440 pts

 	0N to 89.5N --> 179 pts

 	5.01 to 5.01

 	Jan 1958 to Dec 2008 --> 672 pts


 3. Stop Selecting

 4. Click Data Files

 5. Download NetCDF Format
 6. File is too big for GitHub, so compress, mine compressed to 95mb using Keka to 7zip

```{r import SODA temperature data}
cp <- nc_open("/Users/zoekitchel/Documents/grad school/Rutgers/LabWork/Colonization_Extinction/CARTON-GIESE_SODA_v2p1p6_sstemp.nc")
print(cp)

# =========================================
# = Function to Read in SODA, Grab Surface =
# =========================================
get.soda.sst <- function(file){

	soda.info <- nc_open(file)
	name.soda.sizes <- sapply(soda.info$var$temp$dim, function(x)x$name)
	soda.sizes <- soda.info$var$temp$size
	dim.units <- sapply(soda.info$var$temp$dim, function(x)x$units)
	print(dim.units)
	stopifnot(grepl("months since ", dim.units[4])) # make sure time is in correct units and in right place
	names(soda.sizes) <- name.soda.sizes
	ntime <- soda.sizes["time"]
	ndepth <- soda.sizes["depth"]

	soda.time0 <- soda.info$var$temp$dim[[4]]$vals
	ref.date <- as.Date(gsub("months since ", "", dim.units[4]))
	start.before.ref <- grepl("-", soda.time0[1]) # is the first date before ref.date?
	n.month.before <- ceiling(abs(soda.time0[1])) + as.integer(start.before.ref)
	start.increment <- ifelse(start.before.ref, "-1 month", "1 month")
	time.start <- rev(seq.Date(ref.date, by=start.increment, length.out=n.month.before))[1]
	soda.time <- seq.Date(time.start, by="1 month", length.out=ntime)
	
	soda.sst <- brick(file)
	names(soda.sst) <- soda.time

		
	return(soda.sst)
	
}

## Using Ryan's code
soda_sst <- get.soda.sst("/Users/zoekitchel/Documents/grad school/Rutgers/LabWork/Colonization_Extinction/CARTON-GIESE_SODA_v2p1p6_sstemp.nc")
soda_sst

plot(soda_sst[[9]])

saveRDS(soda_sst, "SODA2.1.6_SST.rds")
#load("col_ext/SODA2.1.6_SST.rds")
```

New column in newlatlongyear with "X1958.01.01" format, looks like I may not need to do this after all... SKIPPING FOR NOW
*this takes a super long time to run, so we should save this result
```{r}


newlatlongyear[, month_digits := ifelse(month < 10, paste0("0", month), month)][, day := paste0("0", c(1))][, year_digits := paste0("X", newlatlongyear$year)]
newlatlongyear[, ID := paste(newlatlongyear$year_digits, newlatlongyear$month_digits, newlatlongyear$day, sep=".")]

newlatlongyear <- transform(newlatlongyear, lat = as.numeric(lat), lon = as.numeric(lon))
newlatlongyear[, lat := as.numeric(lat)][, lon := as.numeric(lon)]

vec <- c("ID", "lat", "lon")
newlatlongyearID <- newlatlongyear[, vec, with=FALSE]

save(newlatlongyear, newlatlongyearID, file = "newlatlongyear.Rdata")

```

SET COORDINATE SYSTEM
```{r set coordinate system}
locations <- SpatialPoints(cbind(newlatlongyear$lon, newlatlongyear$lat), 
                            proj4string=CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"))
plot(locations)
```

Now, let's extract values 
*NB: if they do a correct sampling method, this shouldn't be a problem, but is there any world in which we should be averaging temperature values in a different way? here, we're just using the trawl data points, should it be rather the whole region? They seem to be rather arbitrary break points?*

#this also takes a long time, so be sure to save output
```{r extracting temperature values at correct GPS points}
system.time(temp_values <- raster::extract(soda_sst, locations))
save(temp_values, file = "extracted_temp_values_SODA.Rdata")

justlatlong <- newlatlongyear[, vec[2:3], with = FALSE]

df <- cbind(justlatlong, temp_values)
save(df, file = "latlong_temp_values.Rdata")
```
I need a data.table in long format, but right now it's too long, so I'm going back up and only using jan, feb, july, aug.
```{r}
cow <- c("lat", "lon", "*.01.01", "*.02.01", "*.07.01", "*.08.01")

colnames_keep <- grep(paste(cow,collapse="|"), colnames(df), value=TRUE)

df <- df[, colnames_keep, with=FALSE]

df_long <- melt(df, measure.vars = 3:ncol(df),
               variable.name = "dateID", value.name = "temp")

save(df_long, file = "SODA_monthly_data_beforesplit.R")
```

We now have to split the date back up unfortunately, and get rid of X

```{r}
df_long[, c("year", "month", "day") := tstrsplit(dateID, ".", fixed=TRUE)]

df_long2$year <- substr(as.character(df_long2$year), 2, 5)
head(df_long2, 30)
save(df_long2, file = "SODA_monthly_data.R")
```

I currently have mean monthly values for a ton of GPS points within a region.
For temperature values, I will first take a mean for each month over the whole region, so then I have a mean month/year value. region_*month*_mean_sst
Then, I will take mean of mean month/year of year of trawl to get a YEARLY mean to represent the whole region. region_year_mean_sst
For maximum temperature experienced in a year, I will take maximum of mean month/year and for minimum I will take minimum of mean month/year. region_year_max_sst, region_year_min_sst
I intend to look at 10 years of possible lags. 

Therefore, the next step is to merge these data with which regions link to which GPS points using hauls$region

First, I will make a lat lon region key

```{r}
latlongreg.key <- latlongyear[,c(1,2,5)]
```

Then I will link data table with GPS and date and temp with regions using above key, and then take appropriate averages

```{r}
date_temp_reg <- left_join(df_long2, latlongreg.key, by = c("lat", "lon"))
date_temp_reg_month_avg <- date_temp_reg %>%
  group_by(region, year, month) %>%
  summarize(region_month_mean_sst = mean(temp))
head(date_temp_reg_month_avg)

date_temp_reg_avg <- date_temp_reg_month_avg %>%
  group_by(region, year) %>%
  summarize(region_year_mean_sst = mean(region_month_mean_sst), region_year_max_sst = max(region_month_mean_sst), region_year_min_sst = min(region_month_mean_sst)) %>%
  mutate(region_year_seas_sst = region_year_max_sst - region_year_min_sst)
head(date_temp_reg_avg)

save(date_temp_reg_avg, date_temp_reg_month_avg, file = "date_ssttemp_avg_SODA.Rdata") #save this so i can start here!

ggplot(data=date_temp_reg_avg, aes(x=year, y=region_year_mean_sst)) +
  geom_point() +
  facet_wrap(~region)
base::nrow(date_temp_reg_avg)
```

I now have the predictor values I need, yearly min and max for entire region per year

Next step is to calculate change in temp from previous year to now, luckily I can farm this code from 27_acf_timeseries_analysis_covariates.R

I think I should calculate both change in temperature and lags from date_temp_reg_avg in order to keep years not actually surveyed (i.e. triennial west coast survey)

Here I am first adding lags
```{r}
#adding lag values (1, 2, 3, 4, 5) for seasonality and change in temperature
nm1 <- grep("*sst*", colnames(date_temp_reg_avg), value=TRUE)
nm2 <- paste("lag1", nm1, sep=".")
nm3 <- paste("lag2", nm1, sep=".")
nm4 <- paste("lag3", nm1, sep=".")
nm5 <- paste("lag4", nm1, sep=".")
nm6 <- paste("lag5", nm1, sep=".")
nm7 <- paste("lag6", nm1, sep=".")
nm8 <- paste("lag7", nm1, sep=".")
nm9 <- paste("lag8", nm1, sep=".")
nm10 <- paste("lag9", nm1, sep=".")
nm11 <- paste("lag10", nm1, sep=".")
date_temp_reg_avg_withlags <- data.table(date_temp_reg_avg)
date_temp_reg_avg_withlags[, (nm2) :=  data.table::shift(.SD, n=1, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm3) :=  data.table::shift(.SD, n=2, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm4) :=  data.table::shift(.SD, n=3, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm5) :=  data.table::shift(.SD, n=4, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm6) :=  data.table::shift(.SD, n=5, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm7) :=  data.table::shift(.SD, n=6, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm8) :=  data.table::shift(.SD, n=7, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm9) :=  data.table::shift(.SD, n=8, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm10) :=  data.table::shift(.SD, n=9, type = "lag"), by=region, .SDcols=nm1]
date_temp_reg_avg_withlags[, (nm11) :=  data.table::shift(.SD, n=10, type = "lag"), by=region, .SDcols=nm1]
```

Now, I'm adding change since last year
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change") := (region_year_mean_sst - lag1.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change") := (region_year_max_sst - lag1.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change") := (region_year_min_sst - lag1.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change") := (region_year_seas_sst - lag1.region_year_seas_sst)]
```
Change experienced last year
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag1") := (lag1.region_year_mean_sst - lag2.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag1") := (lag1.region_year_mean_sst - lag2.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag1") := (lag1.region_year_mean_sst - lag2.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag1") := (lag1.region_year_mean_sst - lag2.region_year_seas_sst)]
```
Change experienced 2 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag2") := (lag2.region_year_mean_sst - lag3.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag2") := (lag2.region_year_mean_sst - lag3.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag2") := (lag2.region_year_mean_sst - lag3.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag2") := (lag2.region_year_mean_sst - lag3.region_year_seas_sst)]
```
Change experienced 3 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag3") := (lag3.region_year_mean_sst - lag4.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag3") := (lag3.region_year_mean_sst - lag4.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag3") := (lag3.region_year_mean_sst - lag4.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag3") := (lag3.region_year_mean_sst - lag4.region_year_seas_sst)]
```
Change experienced 4 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag4") := (lag4.region_year_mean_sst - lag5.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag4") := (lag4.region_year_mean_sst - lag5.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag4") := (lag4.region_year_mean_sst - lag5.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag4") := (lag4.region_year_mean_sst - lag5.region_year_seas_sst)]
```
Change experienced 5 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag5") := (lag5.region_year_mean_sst - lag6.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag5") := (lag5.region_year_mean_sst - lag6.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag5") := (lag5.region_year_mean_sst - lag6.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag5") := (lag5.region_year_mean_sst - lag6.region_year_seas_sst)]
```
Change experienced 6 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag6") := (lag6.region_year_mean_sst - lag7.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag6") := (lag6.region_year_mean_sst - lag7.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag6") := (lag6.region_year_mean_sst - lag7.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag6") := (lag6.region_year_mean_sst - lag7.region_year_seas_sst)]
```
Change experienced 7 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag7") := (lag7.region_year_mean_sst - lag8.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag7") := (lag7.region_year_mean_sst - lag8.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag7") := (lag7.region_year_mean_sst - lag8.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag7") := (lag7.region_year_mean_sst - lag8.region_year_seas_sst)]
```
Change experienced 8 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag8") := (lag8.region_year_mean_sst - lag9.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag8") := (lag8.region_year_mean_sst - lag9.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag8") := (lag8.region_year_mean_sst - lag9.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag8") := (lag8.region_year_mean_sst - lag9.region_year_seas_sst)]
```
Change experienced 9 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag9") := (lag9.region_year_mean_sst - lag10.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag9") := (lag9.region_year_mean_sst - lag10.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag9") := (lag9.region_year_mean_sst - lag10.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag9") := (lag9.region_year_mean_sst - lag10.region_year_seas_sst)]
```
From Ryan's famous plot, we are more concerned with absolute value of change!
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_abs") := abs(sst_mean_change)]
date_temp_reg_avg_withlags[,c("sst_max_change_abs") := abs(sst_max_change)]
date_temp_reg_avg_withlags[,c("sst_min_change_abs") := abs(sst_min_change)]
date_temp_reg_avg_withlags[,c("sst_seas_change_abs") := abs(sst_seas_change)]
```
Abs Change experienced last year
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag1_abs") := abs(lag1.region_year_mean_sst - lag2.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag1_abs") := abs(lag1.region_year_max_sst - lag2.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag1_abs") := abs(lag1.region_year_min_sst - lag2.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag1_abs") := abs(lag1.region_year_seas_sst - lag2.region_year_seas_sst)]
```
Abs Change experienced 2 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag2_abs") := abs(lag2.region_year_mean_sst - lag3.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag2_abs") := abs(lag2.region_year_max_sst - lag3.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag2_abs") := abs(lag2.region_year_min_sst - lag3.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag2_abs") := abs(lag2.region_year_seas_sst - lag3.region_year_seas_sst)]
```
Abs Change experienced 3 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag3_abs") := abs(lag3.region_year_mean_sst - lag4.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag3_abs") := abs(lag3.region_year_max_sst - lag4.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag3_abs") := abs(lag3.region_year_min_sst - lag4.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag3_abs") := abs(lag3.region_year_seas_sst - lag4.region_year_seas_sst)]
```
Abs Change experienced 4 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag4_abs") := abs(lag4.region_year_mean_sst - lag5.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag4_abs") := abs(lag4.region_year_max_sst - lag5.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag4_abs") := abs(lag4.region_year_min_sst - lag5.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag4_abs") := abs(lag4.region_year_seas_sst - lag5.region_year_seas_sst)]
```
Abs Change experienced 5 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag5_abs") := abs(lag5.region_year_mean_sst - lag6.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag5_abs") := abs(lag5.region_year_max_sst - lag6.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag5_abs") := abs(lag5.region_year_min_sst - lag6.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag5_abs") := abs(lag5.region_year_seas_sst - lag6.region_year_seas_sst)]
```
Abs Change experienced 6 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag6_abs") := abs(lag6.region_year_mean_sst - lag7.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag6_abs") := abs(lag6.region_year_max_sst - lag7.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag6_abs") := abs(lag6.region_year_min_sst - lag7.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag6_abs") := abs(lag6.region_year_seas_sst - lag7.region_year_seas_sst)]
```
Abs Change experienced 7 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag7_abs") := abs(lag7.region_year_mean_sst - lag8.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag7_abs") := abs(lag7.region_year_max_sst - lag8.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag7_abs") := abs(lag7.region_year_min_sst - lag8.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag7_abs") := abs(lag7.region_year_seas_sst - lag8.region_year_seas_sst)]
```
Abs Change experienced 8 years ago
```{r}
date_temp_reg_avg_withlags[,c("sst_mean_change_lag8_abs") := abs(lag8.region_year_mean_sst - lag9.region_year_mean_sst)]
date_temp_reg_avg_withlags[,c("sst_max_change_lag8_abs") := abs(lag8.region_year_max_sst - lag9.region_year_max_sst)]
date_temp_reg_avg_withlags[,c("sst_min_change_lag8_abs") := abs(lag8.region_year_min_sst - lag9.region_year_min_sst)]
date_temp_reg_avg_withlags[,c("sst_seas_change_lag8_abs") := abs(lag8.region_year_seas_sst - lag9.region_year_seas_sst)]
```
Let's git rid of any rows with any NA's (any important ones shouldn't have any) and then save all these goodies!!
```{r}
date_temp_reg_avg_withlags.naomit <- na.omit(date_temp_reg_avg_withlags)
save(date_temp_reg_avg_withlags.naomit, file = "date_ssttemp_avg_SODA_withlags.Rdata")
```

Another way to look at lags, but this isn't appropriate for the binomials I'm currently looking at

```{r}
ccf()
```
